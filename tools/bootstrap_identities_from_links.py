#!/usr/bin/env python3
import json
import glob
import collections
from pathlib import Path
from urllib.parse import urlparse
import requests
from datetime import datetime

OS_HOST = "http://localhost:9200"
INDEX = "onion_identities"

# Pfad zu deinen Link-Dateien
BASE_DIR = Path("/docker/onion-search-starter")
LINK_DIR = BASE_DIR / "data_copy"


def is_onion(host: str | None) -> bool:
    if not host:
        return False
    return host.endswith(".onion")


def extract_domain(url: str) -> str | None:
    try:
        p = urlparse(url)
        host = p.hostname
        if not host:
            return None
        # www. wegschneiden
        if host.startswith("www."):
            host = host[4:]
        return host
    except Exception:
        return None


def load_links():
    """
    Lädt alle links_*.json und baut eine Struktur:
      identities[host] = {
        "darknet_sources": set([...]),
        "clearnet_targets": set([...]),
        "timestamps": [ts1, ts2, ...]
      }
    """
    identities = collections.defaultdict(lambda: {
        "darknet_sources": set(),
        "clearnet_targets": set(),
        "timestamps": []
    })

    for path in LINK_DIR.glob("links_*.json"):
        try:
            with path.open("r", encoding="utf-8") as f:
                doc = json.load(f)
        except Exception as e:
            print(f"[WARN] Konnte {path} nicht lesen: {e}")
            continue

        source = doc.get("source")
        targets = doc.get("targets", [])
        ts = doc.get("ts")  # epoch seconds (so wie in deinem Beispiel)

        if not source:
            continue

        src_parsed = urlparse(source)
        src_host = src_parsed.hostname
        if not src_host:
            continue

        id_entry = identities[src_host]
        id_entry["darknet_sources"].add(source)
        if isinstance(ts, (int, float)):
            id_entry["timestamps"].append(int(ts))

        # Targets klassifizieren
        for t in targets:
            domain = extract_domain(t)
            if not domain:
                continue

            if ".onion" in domain:
                # Weitere Darknet-Targets -> hier könnten wir später mehr machen
                continue
            else:
                # Clearnet-Ziel -> merken
                id_entry["clearnet_targets"].add(t)

    return identities


def build_identity_docs(identities: dict) -> list[dict]:
    docs = []
    for host, data in identities.items():
        timestamps = data["timestamps"] or []
        if timestamps:
            first_seen = min(timestamps)
            last_seen = max(timestamps)
        else:
            # Fallback: jetzt
            now = int(datetime.utcnow().timestamp())
            first_seen = now
            last_seen = now

        # Clearnet-Profile aus Targets bauen (vereinfachtes Modell)
        clearnet_profiles = []
        for t in data["clearnet_targets"]:
            domain = extract_domain(t)
            if not domain:
                continue
            profile = {
                "platform": "generic",           # später: reddit/github/etc.
                "domain": domain,
                "profile_url": t,
                "confidence": 0.5               # Dummy-Wert zum Spielen
            }
            clearnet_profiles.append(profile)

        doc = {
            "identity": host,
            "identity_type": "site",            # später: username/email/handle etc.
            "darknet_sources": sorted(data["darknet_sources"]),
            "clearnet_profiles": clearnet_profiles,
            "emails": [],                       # kann später befüllt werden
            "tags": ["autogenerated_from_links"],
            "first_seen": first_seen,
            "last_seen": last_seen,
        }
        docs.append(doc)
    return docs


def bulk_index(docs: list[dict]):
    """
    Bulk-Import nach OpenSearch
    """
    if not docs:
        print("[INFO] Keine Dokumente zu indexieren.")
        return

    lines = []
    for d in docs:
        lines.append(json.dumps({"index": {"_index": INDEX}}))
        lines.append(json.dumps(d))

    body = "\n".join(lines) + "\n"
    resp = requests.post(
        f"{OS_HOST}/_bulk",
        headers={"Content-Type": "application/x-ndjson"},
        data=body,
        timeout=30,
    )
    if not resp.ok:
        print(f"[ERROR] Bulk-Fehler: {resp.status_code} {resp.text}")
    else:
        rj = resp.json()
        if rj.get("errors"):
            print("[WARN] Bulk-Import mit Fehlern:")
            print(json.dumps(rj, indent=2))
        else:
            print(f"[OK] {len(docs)} Dokumente in {INDEX} indexiert.")


def main():
    print("[*] Lade Link-Daten…")
    identities = load_links()
    print(f"[*] Gefundene Identitäten: {len(identities)}")

    print("[*] Baue Identity-Dokumente…")
    docs = build_identity_docs(identities)
    print(f"[*] Dokumente: {len(docs)}")

    print("[*] Bulk-Index nach OpenSearch…")
    bulk_index(docs)


if __name__ == "__main__":
    main()
